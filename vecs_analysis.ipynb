{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing and Analyzing Subreddit Vector Representations\n",
    "In previous entries I generated vector representations for 1000 subreddits, and saved them to a file. Here I will produce some visualizations and applly k-means clustering to understand community similiarities and group subs. I also pre-reduced the vectors and saved them as 'x' and 'y' columns in their own file for plotting. Note that the dataset includes a surprising number of 'NSFW' subreddits that appear throughout the vector space.\n",
    "\n",
    "## Visualizing All Data\n",
    "Using the amazingly simple plotly library, I produced an interactive view of the vector space. Labels appear next to subreddits showing the subreddit information and colors represent difference in subscriber numbers (default subreddits appear red) The interactive plot can be found [here](https://plot.ly/~rlindsay22/2/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.plotly as py\n",
    "\n",
    "vecs_df = pd.read_csv('data/notebook_support/vecs.csv')\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x = vecs_df['x'],\n",
    "    y = vecs_df['y'],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 6,\n",
    "        color = vecs_df['subscribers'],\n",
    "        colorscale='magma',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = vecs_df['name']\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "#py.iplot(data, filename='subreddits')\n",
    "# commented out to not overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Interactive Plot:\n",
    "![Subreddit Vectors with Small-Medium Dataset](figures/subreddit_vecs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Just by looking, the data appears to have significant meaningful clustering. An interesting characteristic to note is the spread out nature of the default subreddits, which makes this representation unique from interest-mapping approaches. Each default sureddit is close to subs that it shares discussion features with, rather than user engagement. There are distinct and logical bubbles such as sports, videogames, TV etc. that supposedly justify some of the lesser-obvious groupings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Next I performed clustering using the scipy implementation of k-means. I decided to run the algorithm with initialized centroids corresponding to my best guesses at where clusters would lie, but it ended up being difficult to isolate the clusters that were actually visible in the data from the beginning, so k-means might not be the best method. I'll include the code and plot here nonetheless (I ended up plotting with k=12, but you'll notice that some of the 'clusters' are fairly meaningless. The code shown initializes k-means with k=12, rather than with centroids. Note that in the following sections, I have decided to perform clustering **on the pre-reduced data**, as I believe it retains the amount of meaning I want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans as km\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "def get_assignments(X, k):\n",
    "    cents,dist = km(X,k) # run k-means and get centroids\n",
    "    m = X.shape[0]\n",
    "    c_vals = np.zeros((m,1)) # array to hold centroid assignments\n",
    "    for i in range(0,m):\n",
    "        row = X[i,:]\n",
    "        diff = cents - row\n",
    "        diff_vals = np.zeros((k,1))\n",
    "        for c in range(0,k):\n",
    "            diff_vals[c] = la.norm(diff[c,:])\n",
    "        c_vals[i] = np.argmin(diff_vals)\n",
    "    return cents, c_vals\n",
    "\n",
    "vecs = pd.read_csv('data/notebook_support/vecs.csv',usecols=[1,2])\n",
    "X = vecs.values\n",
    "k = 12\n",
    "cents, c_vals = get_assignments(X, k)\n",
    "\n",
    "vecs_df['cluster'] = c_vals\n",
    "vecs_df.to_csv('data/notebook_support/vecs.csv', encoding='utf-8', columns=['name','x','y','subscribers','cluster'], index=False)\n",
    "\n",
    "# save the centroids\n",
    "centroid_data = {'x': cents[:,0], 'y': cents[:,1]}\n",
    "cents_df = pd.DataFrame(centroid_data,columns=['x','y'])\n",
    "cents_df.to_csv('data/notebook_support/centroids.csv',encoding='utf-8',columns=['x','y'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the clustering, I'll plot the data with colours corresponding to assignments. I use a manually-created colour map for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# map each cluster assignment to a colour\n",
    "color_map = {\n",
    "    0.0: '#e6f2ff', 1.0: '#99ccff', \n",
    "    2.0: '#ccccff', 3.0: '#cc99ff', \n",
    "    4.0: '#ff99ff', 5.0: '#ff6699', \n",
    "    6.0: '#ff9966', 7.0: '#ff6600',\n",
    "    8.0: '#ff5050', 9.0: '#ff0000',\n",
    "    10.0: '#18ff01', 11.0: '#6a2b11',\n",
    "    12.0: '#b7bf05', 13.0: '#2859c1'}\n",
    "    # supports up to 14 clusters\n",
    "\n",
    "def cluster_plot_data(vecs_df, cents):\n",
    "    cols = vecs_df['cluster'].map(color_map)\n",
    "\n",
    "    data_points = go.Scatter(\n",
    "        name = 'subreddits',\n",
    "        x = vecs_df['x'],\n",
    "        y = vecs_df['y'],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 6,\n",
    "            color = cols,\n",
    "        ),\n",
    "        text = vecs_df['name']\n",
    "    )\n",
    "\n",
    "    centroid_plot = go.Scatter(\n",
    "        name = 'centroids',\n",
    "        x = cents[:,0],\n",
    "        y = cents[:,1],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 6,\n",
    "            color = 'black',\n",
    "            symbol = 1\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    data = [data_points,centroid_plot]\n",
    "    return data\n",
    "\n",
    "data = cluster_plot_data(vecs_df, cents)\n",
    "\n",
    "#py.iplot(data, filename='subreddit-clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Manually Initialize Clusters](figures/clusters_init.png)\n",
    "And here is [the interactive plot](https://plot.ly/~rlindsay22/4/subreddits-vs-centroids/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Improving Things\n",
    "In order tighten things up and get a better workflow and data-representation, I rewrote the download script to filter nsfw subreddits among other new functions and downloaded the top 10,000 subreddits. (Around 8000 were saved, with the remaining ~2000 being skipped for nsfw or exclusivity reasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium Dataset Plot\n",
    "[Here](https://plot.ly/~rlindsay22/8/) is the plot for the medium-sized dataset. Subscribers are coloured on a logarithmic scale due to the skewed distribution of subscriber counts.\n",
    "\n",
    "This plot preserves the relevant groupings seen before, but of course in higher density. Some new groupings appear with this amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_df = pd.read_csv('data/dataset_medium/vecs.csv')\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x = vecs_df['x'],\n",
    "    y = vecs_df['y'],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 6,\n",
    "        color = np.log(vecs_df['subscribers']),\n",
    "        colorscale='Viridis'\n",
    "    ),\n",
    "    text = vecs_df['name']\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "#py.iplot(data, filename='subreddits_medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Subreddits Medium Dataset with Logarithmic Colorscale](figures/subreddits_medium.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters\n",
    "I decided to give k-means another shot. This time, I did not manually initialize the centroids but instead let the algorithm determine them to start. Once again, I store the centroids and example assignments in the centroids and vector files, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = pd.read_csv('data/dataset_medium/vecs.csv',usecols=[1,2])\n",
    "X = vecs.values\n",
    "k = 14\n",
    "cents, c_vals = get_assignments(X, k)\n",
    "\n",
    "# save the things\n",
    "vecs_df['cluster'] = c_vals\n",
    "vecs_df.to_csv('data/dataset_medium/vecs.csv', encoding='utf-8', columns=['name','x','y','subscribers','cluster'], index=False)\n",
    "centroid_data = {'x': cents[:,0], 'y': cents[:,1]}\n",
    "cents_df = pd.DataFrame(centroid_data,columns=['x','y'])\n",
    "cents_df.to_csv('data/dataset_medium/centroids.csv',encoding='utf-8',columns=['x','y'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, I ended up sticking with the arbitrary k=14 (the maximum number of color assignments I have). [The plot](https://plot.ly/~rlindsay22/14/subreddits-vs-centroids/) is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cluster_plot_data(vecs_df, cents)\n",
    "#py.iplot(data, filename='subreddit-clusters-medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Medium Dataset Clusters](figures/clusters_medium.png)\n",
    "\n",
    "### Final Note on K-Means Clustering\n",
    "While k-means effectively groups and minimizes distance (as it should) with this dataset, it often fails to separate the smaller 'modules' that represent specific content circles. Furthermore, this method does not group subreddits in the same way a human would just by looking at the plot and the subreddit themselves, but this is not the objective of k-means. K-means clustering in this case ends up being a way of dividing the data into distance-minimized, but rather meaningless, chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "Having seen and gained a decent understanding of the data, my next steps will be to perform deeper analysis on the community circles and hopefully better quantify the connections between subreddits, as well as to use the vector representations in a recommender-like system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

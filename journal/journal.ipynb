{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation and Progress Journal\n",
    "This notebook serves as a log of analysis, experimentation, and development performed on the recommender models. It will be updated when and if I do something noteworthy in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Analysis\n",
    "As an early step to get my bearings on things, I wrote a praw downloader and downloaded 10 top subreddits' content from 1000 of the last comments. After selecting top occurring words and counting them accross subreddits, I simply mean-normalized the counts and produced the following plot from from a pandas DataFrame (Note that the y-axis is the normalized feature and the x-axis reflects the arbitrary vocabulary list selected from the entire content set):\n",
    "![Word Count Sample Plot](ex_content_features1.png)\n",
    "Right off the bat, we can see a cluster of huge spikes toward the left of the plot, likely corresponsding to high-frequency words that are common to all subreddits. Also note that the entire plot is decreasing due to the sorted state of the word counts in the dataset.\n",
    "\n",
    "For a little sanity-check analysis, I selected a few of the smaller spikes in the 'word 100' to 'word 200' range and cross-references them against the count data and vocabulary list. As expected, subreddits like r/gadgets and r/news had high refquencies of 'phone' and 'cop' respectively. So at least this seems to work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

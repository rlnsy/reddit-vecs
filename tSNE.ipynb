{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying TFIDF Vectors and t-SNE to Subreddit Content\n",
    "While researching for this project, I discovered a whole lot of subreddit mappings and visualaztions out there. Most of them employ user engagement as a metric for modularity detection. (Insert Sources). Since this project is based on breaking free of traditional user interest-patterns, I decided to quantify subreddit differences by content alone, in much the same way as Andrej Kaparthy does in [this blog post](http://karpathy.github.io/2014/07/02/visualizing-top-tweeps-with-t-sne-in-Javascript/).\n",
    "\n",
    "## Collecting Data\n",
    "Using the previously mentioned post as a rough guide, I started implementing the community difference analysis. Initial analysis, I simply downloaded comment text from a large handful of default subreddits, with the code saved in '/download.py'. Basically I just save a file for each sub with concatenated raw comment text, and a list of the files (for scikit to use later on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/gadgets.txt', 'data/sports.txt', 'data/gaming.txt', 'data/pics.txt', 'data/worldnews.txt', 'data/videos.txt', 'data/AskReddit.txt', 'data/aww.txt', 'data/funny.txt', 'data/news.txt', 'data/movies.txt', 'data/blog.txt', 'data/books.txt', 'data/history.txt', 'data/food.txt', 'data/philosophy.txt', 'data/Jokes.txt', 'data/Art.txt', 'data/DIY.txt', 'data/space.txt', 'data/Documentaries.txt', 'data/askscience.txt', 'data/nottheonion.txt', 'data/todayilearned.txt', 'data/personalfinance.txt', 'data/gifs.txt', 'data/listentothis.txt', 'data/IAmA.txt', 'data/announcements.txt', 'data/TwoXChromosomes.txt', 'data/creepy.txt', 'data/nosleep.txt', 'data/GetMotivated.txt', 'data/WritingPrompts.txt', 'data/LifeProTips.txt', 'data/EarthPorn.txt', 'data/explainlikeimfive.txt', 'data/Showerthoughts.txt', 'data/Futurology.txt', 'data/photoshopbattles.txt', 'data/mildlyinteresting.txt', 'data/dataisbeautiful.txt', 'data/tifu.txt', 'data/OldSchoolCool.txt', 'data/UpliftingNews.txt', 'data/InternetIsBeautiful.txt', 'data/science.txt']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "files_store = open('data/files.pickle','rb')\n",
    "files_list = pickle.load(files_store)\n",
    "print(files_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "Next I used the TfidfVectorizer from scikit to process and vectorize the content based on text features. These vectors are dimensioned according to the number of ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(input='filename',stop_words='english',lowercase=True, strip_accents='unicode', smooth_idf=True,sublinear_tf=False, use_idf=True, ngram_range=(1,2),min_df=2)\n",
    "vecs = vectorizer.fit_transform(files_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a 47 X 35155 matrix where each row is a subreddit TFIDF matrix with horizontal dimension corresponding to the ngram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<47x35155 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 175068 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "The next step is to apply t-SNE implementations and see how they work on the data. I used scikit's built-implementation for this task. For t-SNE the the dot product dissimilarities are require as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = (vecs * vecs.T).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -45.519707 ,  -16.77306  ],\n",
       "       [-125.85451  ,  -63.12555  ],\n",
       "       [ -25.442345 ,  -66.13916  ],\n",
       "       [ -56.59373  ,  -65.23105  ],\n",
       "       [  71.441795 ,   10.851768 ],\n",
       "       [  21.26784  ,  -10.18458  ],\n",
       "       [   5.372902 ,  -53.599323 ],\n",
       "       [ -62.719513 , -100.67452  ],\n",
       "       [  23.913986 ,  -76.69058  ],\n",
       "       [  46.01973  ,   16.321417 ],\n",
       "       [ -11.167515 , -118.654045 ],\n",
       "       [ -69.46114  ,   32.624287 ],\n",
       "       [  18.797682 , -120.17274  ],\n",
       "       [ -35.36316  ,   44.121063 ],\n",
       "       [-124.051384 ,   17.163965 ],\n",
       "       [  85.13571  ,  -60.6074   ],\n",
       "       [-108.79093  ,   43.000973 ],\n",
       "       [  -4.93069  ,   91.66717  ],\n",
       "       [ -39.328228 ,  141.07199  ],\n",
       "       [ -77.78647  ,    1.950279 ],\n",
       "       [  50.574123 ,  -14.068214 ],\n",
       "       [ -39.298004 ,  110.72812  ],\n",
       "       [  27.72952  ,   58.529602 ],\n",
       "       [   0.5471846,  -87.81489  ],\n",
       "       [  30.232647 ,  -40.019    ],\n",
       "       [ -25.176321 ,  -35.995033 ],\n",
       "       [ -59.670567 ,   88.703476 ],\n",
       "       [  86.684715 ,   38.427357 ],\n",
       "       [  84.6682   ,  -23.429485 ],\n",
       "       [  72.86371  ,  -88.780624 ],\n",
       "       [ -90.00989  ,  -76.937256 ],\n",
       "       [ -29.166595 ,   74.01785  ],\n",
       "       [  43.325848 ,  -98.55188  ],\n",
       "       [ -85.28947  ,   64.07285  ],\n",
       "       [ -24.67689  ,   14.660552 ],\n",
       "       [  10.697132 ,   34.38989  ],\n",
       "       [  -1.6643703,  -23.550762 ],\n",
       "       [  -1.4393011,    6.807224 ],\n",
       "       [  61.315624 ,  -40.663437 ],\n",
       "       [ -72.49119  ,  114.629745 ],\n",
       "       [  -8.072621 ,   52.180687 ],\n",
       "       [ -95.34956  ,   92.623276 ],\n",
       "       [-128.30557  ,  -11.3066025],\n",
       "       [ -31.58844  ,  -95.21059  ],\n",
       "       [  49.28935  ,  -65.0026   ],\n",
       "       [ -75.46874  ,  143.87035  ],\n",
       "       [-104.723724 ,  -36.19594  ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "vecs_2d = TSNE(n_components=2).fit_transform(dists)\n",
    "vecs_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have (supposedly meaningful) 2-dimensional vector representations of all the subreddits!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
